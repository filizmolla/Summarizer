 Today, we're going to be going through the seven most important data structures and explaining them as simply as possible. These are extremely important to learn whether it's for coding interviews, computer science class, or building projects. We'll be going through the list from easiest to hardest, so beginners have a better idea of where to begin. We'll go over a simplified explanation for what each data structure is and talk about common uses for that data structure. I'll also be putting the time complexity for common operations on screen, but if you don't know what that is, don't worry. I'm just including it on the screen for the people who want to see it. Let's not waste any time and get right into it with our first data structure. Arrays are ordered collections of data. Typically, the data is all of a similar type, like integers or strings, but some languages also allow for different data types. An example of a real-life use for an array would be if you had temperatures for the next five days and wanted to store them so your program could access them later. Arrays are used all the time and for pretty much everything, so they're definitely one of the most important data structures to learn first. One of the amazing benefits of using an array is that it's very easy to find any element. As each element in an array is assigned a number called an index that you can use to find it. This form of numbering is often called zero-based indexing, which it just means the first element in the array is added index of zero. This can often confuse new programmers because if you want to retrieve the second element, you have to use its index, which is one, not two. Well, the advantage of arrays is that it makes it easy to read elements. The disadvantage is that they have a slightly harder time inserting or deleting elements. Now, just a quick note. We won't talk about memory much in this video, but for the first two data structures, the memory component is very simple and it's essential that you know it so you can tell the difference between them. We'll use our temperature array from earlier as an example. Arrays are stored in contiguous memory, which means each of the elements in the array are next to one another in memory. If a new element is added in the middle of the array, the entire array must shift down. But what would have happened if there was already something in the next memory address? To fit this new element in now, the arrays memory will have to be reallocated to an entirely new space where all of the elements fit. While computers are incredibly quick, this is not very efficient. So, to summarize, arrays are very good for reading elements, but can be a bit less efficient when it comes to insertion or deletion. Now, we'll take a look at a data structure if that's the opposite. While arrays are fast at reading elements and a bit slower at inserting or deleting, linked lists are a bit slower at reading elements, but fast at inserting or deleting. Lint lists are similar to arrays in that they also store ordered lists of data elements. However, a huge difference is in the way they are stored in memory. Each element of a linked list has what we call a pointer, which is basically the address of the next element of the list. As a result, elements in a linked list do not have to be stored beside each other. You can store the next element in any location and the previous element will point to it if you want to find it. The advantage of this is that it solves our problem with array insertions and deletions. To add a new element, you find a free spot in memory for it and have the previous element point to it. To remove an element, you just delete the element and have the previous element point to the one ahead of the element you just deleted. The disadvantage of using linked lists is that they do not have indexes as the elements are not stored right beside each other. This means that to find an element, we have to go through the list starting from the beginning. If we want the third element, we have to first look at the first element, see where it points, go to the second, see where it points, and then we find the third element. If you think of a huge linked list, you can start to understand why it's not the fastest at reading. So to recap, arrays are faster when it comes to reading, linked lists are faster when it comes to inserting and deleting. Remember how arrays had value stored and for each value, there was an index that numbered them? Well, hash maps are essentially the same thing, except that you can choose what the index is, which hash maps call a key. The key and its value are commonly known as key value pairs. The other major difference between arrays and hash maps is that hash maps are unordered. Hash maps are fast for both inserting and removing elements, but the real benefit to using hash maps is their ability to search quickly. Let's say you wanted to store a list of capital cities. If we stored these in an array, we would have to know the index for each one to read it. But for a hash map, if we make the keys countries, we can just look up the country to find the capital city associated with it. Hash maps go by a few different names. They are sometimes called hash tables, or if you know Python, they're called dictionaries. For the purposes of this video, you can assume they're all the same thing. So, if you know how to use dictionaries in Python, congratulations. You've actually been using hash maps the entire time. The way hash maps work underneath the hood is really interesting, but a bit out of the scope of this video. I'll be making specific videos for all of the data structures, so we will cover hash maps more in depth there. For now, just understand that they're unordered and their custom keys allow for very quick searching. The most simple way to describe stacks is to think of a stack of plates or pancakes. The first plate goes on the bottom of the pile, and the last plate goes on the top. Stacks are called life-o structures, which stands for last in first out. Because the last element in is like the last plate that goes on the top of the stack. When you go to grab a plate, the last plate on top will be the first one you take off. Stacks have three common operations, which are push, pop, and peak. Pushing is when you add a new element to the top of the stack. Popping is when you remove the topmost element from the stack, and peaking is when you're just taking a look at what the element at the top of the stack is. All of these are very fast, which is why stacks are optimal for certain problems. If you're wondering when stacks might be used, think of the pancake and plate examples. Any scenario that has a similar structure where the last element in is the first element that should come out, stacks are likely a very good data structure to use. Cues are the opposites of stacks. The simplest way to think of a queue is like a lineup at a grocery store. The first person in the line will get serviced first, and every additional person who joins the line goes at the end. Cues are fight-fo structures, which stands for first in first out, because the first element in line is the first element to come out. Cues have very similar operations to stacks, which are called NQ, DQ, and Front. NQ is like push for a stack, and is when a new element is added to the back of the queue. DQ is like pop for a stack, and is when the element on the front of the queue is removed. And Front is like peak for a stack, and is when you take a look at the front most element in the queue. Cues are often more frequently used than stacks, especially in real world programming. Think of something like YouTube playlists. When you start watching a playlist, you'll start with the video that was added first, and the last video you watch will be the final one that was added. Trees are a category of data structure that, as you might have guessed by the name, resemble trees. Trees have nodes, which are connected to each other by edges. The first node in a tree is called the root node. Nodes have a parent child direction, where one node is a parent node that leads to another node, which is called a child node. Sometimes, people will refer to parent nodes as just regular nodes, and the child nodes as leaves. There are tons of tree-based data structures, but in this video, we'll be talking about binary trees, and in particular, binary search trees. A binary tree is a tree where the parent node has up to two children nodes. A binary search tree is a type of binary tree, where all children nodes on the left are less than the parent node, and all right children nodes are greater than the parent node. These binary search trees make it very easy to search through large amounts of ordered values. The classic example is to think of a number guessing game, where one person thinks of a number between one and a hundred, and the other person has to guess. With each guess, they get told whether the correct number is higher or lower than their guess. The strategy is to always guess the middle number, because then you're eliminating the most amount possible each time. This is what a binary search tree does. We can eliminate a parent node and everything below it or above it, and continue that process until we get to our correct number. This is not only useful for this game though. A more practical example is to think of a digital dictionary. Dictionaries have over a hundred thousand words. If you give a computer a word and want it to give you back the definition, it would be incredibly slow for it to start at the beginning and search through every word until it finds the correct one. Instead, because the dictionary is sorted alphabetically, the computer is able to go right to the word in the middle of the dictionary, and check if the target word comes before or after it. It continues sorting like this until it reaches the word. There are tons of other tree-like structures like heaps and tries, but we'll be saving those for another video. Now onto our final data structure. Lastly, we have graphs. Graphs are basically models for a set of connections. Like trees, graphs are made up of nodes and edges. In fact, trees, and even linked lists to an extent, are technically just types of graphs. But graphs can get a lot more complicated than that. In a graph, there are far less restrictions than with trees. Nodes can be connected to any amount of neighbors. Graphs can be directed, where nodes point other nodes, but they can also be undirected. Some graphs have cycles, where two nodes point to each other. The edges between nodes can also be weighted, where the path has a value associated with it. As you can see, graphs are very complicated, which is why many people consider them to be one of the hardest data structures to learn. I'm going to make an entirely separate video dedicated to graphs, so we'll tackle the complexities in a little bit. For now, let's see an example where graphs are useful data structures. Imagine you're running errands, and you have five different stores to visit. We can represent this as a graph, where each store is a node, and each edge has a distance. Using this data structure, we can develop an algorithm that allows us to calculate the shortest route between all five places. For a real life example, think of Uber. Every Uber driver and user could be seen as nodes, and the application is constantly trying to optimize, so that the waiting time for each rider is as short as possible. There are endless applications to graphs, which is why there's such an important data structure to understand. Thanks so much for watching. If you enjoyed this content on data structures, and want to see more data structure videos, comment down below and leave a like on the video. I want to take a quick moment and express my gratitude to everyone for being so incredibly supportive so far, and to celebrate hitting 1000 subscribers. I started this channel just over a month ago, and I've been super fortunate to have found 1000 amazing people who've been willing to take a chance on my small YouTube channel. I never thought I'd be able to hit 100 subscribers in this time, let alone 1000, so I'm just so thankful that all of you guys have been so supportive. I promise I'll do my absolute best to continue increasing the quality and quantity of content as much as I can. Thank you, and 10,000, here we come.