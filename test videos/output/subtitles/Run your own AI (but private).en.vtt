WEBVTT
Kind: captions
Language: en

00:00:00.090 --> 00:00:03.480
I'm running something called private
ai. It's kind of like chat GPT,

00:00:03.540 --> 00:00:07.290
except it's not. Everything about it
is running right here on my computer.

00:00:07.440 --> 00:00:08.820
Am I even connected to the internet?

00:00:08.970 --> 00:00:12.810
This is private contained and my data
isn't being shared with some random

00:00:12.815 --> 00:00:15.510
company. So in this video I
want to do two things. First,

00:00:15.570 --> 00:00:16.800
I want to show you how to set this up.

00:00:16.890 --> 00:00:21.240
It is ridiculously easy and fast to run
your own AI on your laptop computer or

00:00:21.240 --> 00:00:23.580
whatever. It's this is free, it's amazing.

00:00:23.580 --> 00:00:25.980
It'll take you about five minutes and
if you stick around until the end,

00:00:26.040 --> 00:00:28.410
I want to show you something even
crazier, a bit more advanced.

00:00:28.560 --> 00:00:31.380
I'll show you how you can connect
your knowledge base, your notes,

00:00:31.380 --> 00:00:32.280
your documents,

00:00:32.340 --> 00:00:37.080
your journal entries to your own
private GPT and then ask it questions

00:00:37.080 --> 00:00:38.670
about your stuff. And then second,

00:00:38.730 --> 00:00:42.480
I want to talk about how private AI is
helping us in the area we need help Most.

00:00:42.510 --> 00:00:44.160
Our jobs, you may not know this,

00:00:44.165 --> 00:00:47.460
but not everyone can use chat GBT
or something like it at their job.

00:00:47.610 --> 00:00:51.030
Their companies won't let them mainly
because of privacy and security reasons,

00:00:51.180 --> 00:00:54.810
but if they could run their own
private ai, that's a different story.

00:00:54.810 --> 00:00:58.200
That's a whole different ballgame and
VMware is a big reason. This is possible.

00:00:58.230 --> 00:01:01.200
They're the sponsor of this video and
they're enabling some amazing things that

00:01:01.200 --> 00:01:05.250
companies can do on-Prem in their
own data center to run their own ai.

00:01:05.400 --> 00:01:07.890
And it's not just the cloud man,
it's like in your data center.

00:01:07.980 --> 00:01:10.740
The stuff they're doing is crazy. We're
going to talk about it here in a bit,

00:01:10.920 --> 00:01:13.320
but tell you what, go ahead and do
this. There's a link in the description.

00:01:13.470 --> 00:01:16.020
Just go ahead and open it and take a
little glimpse at what they're doing.

00:01:16.110 --> 00:01:16.920
We're going to dive deeper,

00:01:16.920 --> 00:01:20.070
so just go ahead and have it open right
in your second monitor or something or

00:01:20.075 --> 00:01:22.200
on the side or minimize. I
don't know what you're doing.

00:01:22.200 --> 00:01:25.080
I dunno how many monitors you
have. You have three Actually, Bob,

00:01:25.170 --> 00:01:27.570
I can see before we get started,
I have to show you this.

00:01:27.600 --> 00:01:31.230
You can run your own private ai. That's
kind of uncensored. I watch this,

00:01:34.770 --> 00:01:37.560
So yeah, please don't do
this to destroy me. Also,

00:01:37.620 --> 00:01:39.750
make sure you're paying attention
at the end of this video,

00:01:39.750 --> 00:01:42.780
I'm doing a quiz and if you're one of
the first five people to get a hundred

00:01:42.780 --> 00:01:46.260
percent on this quiz, you're getting
some free coffee network. Chuck Coffee.

00:01:46.440 --> 00:01:49.110
So take some notes,
study up. Let's do this

00:01:51.030 --> 00:01:54.750
now real quick, before we install a
private local AI model on your computer,

00:01:55.440 --> 00:01:58.440
what does it even mean? What's
an AI model? At its core,

00:01:58.440 --> 00:02:02.520
an AI model is simply an artificial
intelligence pre-trained on data we

00:02:02.520 --> 00:02:05.730
provided. One you may have
heard of is open AI's Chat GBT,

00:02:05.880 --> 00:02:08.370
but it's not the only one out
there. Let's take a field trip.

00:02:08.430 --> 00:02:11.250
We're going to go to a website
called hugging face.co.

00:02:11.520 --> 00:02:13.920
Just an incredible brand
name. I love it so much.

00:02:14.100 --> 00:02:18.810
This is an entire community dedicated
to providing and sharing AI models and

00:02:18.810 --> 00:02:21.330
there are a ton. You're about
to have your mind blown. Ready?

00:02:21.600 --> 00:02:26.220
I'm going to click on models up here. Do
you see that number? 505,000 AI models.

00:02:26.310 --> 00:02:30.210
Many of these are open and free
for you to use and pre-trained,

00:02:30.240 --> 00:02:32.250
which is kind of a crazy
thing. Let me show you this.

00:02:32.280 --> 00:02:35.250
We're going to search for
a model named Llama two,

00:02:35.670 --> 00:02:39.510
one of the most popular models out
there. We'll do LAMA two seven B. Again,

00:02:39.510 --> 00:02:40.343
I love the branding.

00:02:40.560 --> 00:02:44.820
LAMA two is an AI model known as
an LLM or large language model,

00:02:45.000 --> 00:02:48.420
open AI's Chat. GPT is
also an LLM. Now this LLM,

00:02:48.425 --> 00:02:51.060
this pre-trained AI
model was made by meda,

00:02:51.570 --> 00:02:54.540
AKA Facebook and what
they did to pre-train.

00:02:54.540 --> 00:02:58.080
This model is kind of insane and the fact
that we're about to download this and

00:02:58.085 --> 00:03:01.720
use it even crazier, check this out
if you scroll down just a little bit,

00:03:01.960 --> 00:03:02.980
here we go. Training data.

00:03:03.130 --> 00:03:07.360
It was trained by over 2 trillion
tokens of data from publicly available

00:03:07.360 --> 00:03:11.770
sources. Instruction data sets over
a million human annotated examples,

00:03:11.860 --> 00:03:15.100
data freshness. We're talking
in July, 2023. I love that term.

00:03:15.790 --> 00:03:18.490
Data freshness and getting
the data was just step one.

00:03:18.550 --> 00:03:21.340
Step two is insane because this
is where the training happens.

00:03:21.370 --> 00:03:25.150
Mata to train this model put together
what's called a super cluster.

00:03:25.270 --> 00:03:29.530
It already sounds cool, right?
This sucker is over 6,000 GPUs.

00:03:29.620 --> 00:03:34.360
It took 1.7 million GPU hours to
train this model and it's estimated it

00:03:34.420 --> 00:03:39.070
costs around $20 million to train
it and now made is just like,

00:03:39.190 --> 00:03:43.390
here you go kid. Download this
incredibly powerful thing.

00:03:43.450 --> 00:03:46.030
I don't want to call it a being
yet. I'm not ready for that,

00:03:46.180 --> 00:03:50.230
but this intelligent source of information
that you can just download on your

00:03:50.230 --> 00:03:51.850
laptop and ask it questions,

00:03:51.970 --> 00:03:55.600
no internet required and this is just
one of the many models we could download.

00:03:55.600 --> 00:03:58.810
They have special models like
text to speech, image to image.

00:03:58.960 --> 00:04:02.350
They even have uncensored ones. They have
an uncensored version of a llama too.

00:04:02.530 --> 00:04:03.820
This guy George Sung,

00:04:04.150 --> 00:04:07.960
took this model and fine tuned
it with a pretty hefty GPU,

00:04:08.440 --> 00:04:11.920
took him 19 hours and made it to where
you could pretty much ask this thing.

00:04:11.950 --> 00:04:14.350
Anything you wanted, whatever
question comes to mind,

00:04:14.620 --> 00:04:16.000
it's not going to hold back. Okay,

00:04:16.060 --> 00:04:19.240
so how did we get this fine tuned
model onto your computer? Well,

00:04:19.240 --> 00:04:22.660
actually I should warn you, this
involves quite a bit of llamas,

00:04:22.720 --> 00:04:26.260
more than you would expect. Our
journey starts at a tool called O Lama.

00:04:26.350 --> 00:04:28.000
Let's go ahead and take a field
trip out there real quick.

00:04:28.150 --> 00:04:32.350
We'll go to O lama.ai. All we'll have
to do is install this little guy, Mr.

00:04:32.350 --> 00:04:32.950
Alama,

00:04:32.950 --> 00:04:37.930
and then we can run a ton of different
LLMs Llama two Code Llama told you lots

00:04:37.935 --> 00:04:41.170
of llamas and there's others that are
pretty fun like Llama two Uncensored or

00:04:41.170 --> 00:04:46.150
Llamas. Tdrl. I'll show you in a second.
But first, what do we install alama on?

00:04:46.330 --> 00:04:49.480
We can see right down here that we
have it available on macOS and Linux,

00:04:49.480 --> 00:04:52.150
but oh bummer, windows coming soon.

00:04:52.240 --> 00:04:56.650
It's okay because we've got WSL,
the Windows subsystem for Linux,

00:04:56.710 --> 00:04:58.210
which is now really easy to set up.

00:04:58.330 --> 00:05:00.610
So we'll go ahead and click on
download right here from os.

00:05:01.210 --> 00:05:04.060
You'll just simply download this
and install like one of your regular

00:05:04.065 --> 00:05:06.460
applications for Linux.
We'll click on this.

00:05:07.000 --> 00:05:09.970
We got to fun curl command that will
copy and paste now because we're going to

00:05:09.970 --> 00:05:14.620
install WSL on Windows. This will
be the same step. So Mac OS folks,

00:05:15.220 --> 00:05:19.180
go ahead and just run that installer.
Linux and Windows folks, let's keep going.

00:05:19.450 --> 00:05:20.283
Now, if you're on Windows,

00:05:20.500 --> 00:05:23.920
all you have to do now to get WSL
installed is launch your Windows terminal.

00:05:23.950 --> 00:05:27.910
Just go to your search bar and search
for terminal and with one command it'll

00:05:27.910 --> 00:05:31.810
just happen. It used to be so much
harder, which is WSL dash dash install.

00:05:32.500 --> 00:05:35.620
It'll go through a few steps.
It'll install Ubuntu as default.

00:05:35.800 --> 00:05:39.280
I'll go ahead and let that do
that. And boom, just like that.

00:05:39.370 --> 00:05:44.110
I've got Ubuntu 22 0 4 3 lts installed
and I'm actually inside of it right

00:05:44.110 --> 00:05:47.260
now. So now at this point, Linux
and Windows folks, we converged.

00:05:47.260 --> 00:05:49.270
We're on the same path.
Let's install alama.

00:05:49.480 --> 00:05:51.610
I'm going to copy that curl
command that alama gave us,

00:05:52.000 --> 00:05:54.970
jump back into my terminal, paste
that in there and press enter.

00:05:55.420 --> 00:05:59.330
Fingers crossed, everything should be
great. Like the way it is right now,

00:05:59.390 --> 00:06:04.250
it'll ask for my pseudo password and
that was it. Oh, LAMA is now installed.

00:06:04.310 --> 00:06:07.190
Now this will directly apply to
Linux people and Windows people.

00:06:07.250 --> 00:06:10.520
See right here where it says Nvidia
GPU installed. If you have that,

00:06:10.520 --> 00:06:13.310
you're going to have a better time
than other people who don't have that.

00:06:13.460 --> 00:06:15.830
I'll show you here in a second.
If you don't have it, that's fine.

00:06:15.830 --> 00:06:18.650
We'll keep going. Now let's run an
LLM. We'll start with llama two.

00:06:18.800 --> 00:06:22.040
So we'll simply type in, oh Lama run,

00:06:22.130 --> 00:06:26.870
and then we'll pick one llama
two and that's it. Ready,

00:06:26.870 --> 00:06:28.790
set go. It's going to pull the manifest.

00:06:28.910 --> 00:06:31.010
It'll then start pulling down
and downloading Llama two.

00:06:31.010 --> 00:06:34.160
And I want you to just realize this,
that powerful LAMA two pre-training,

00:06:34.165 --> 00:06:38.210
we talked about all the money and
hours spent. That's how big it is.

00:06:38.510 --> 00:06:41.690
This is the 7 billion
parameter model or the seven B.

00:06:42.230 --> 00:06:45.290
It's pretty powerful and we're about to
literally have this in the palm of our

00:06:45.290 --> 00:06:49.880
hands in like 3, 2, 1. Oh,
I thought I had it. Anyways,

00:06:49.940 --> 00:06:51.980
it's almost done. And boom, it's done.

00:06:52.160 --> 00:06:56.300
We've got a nice success message
right here and it's ready for us.

00:06:56.450 --> 00:06:58.760
We can ask you anything.
Let's try what is a pug?

00:06:59.210 --> 00:07:01.490
Now the reason this is going
so fast, just like a side note,

00:07:01.760 --> 00:07:05.180
is that I'm running A GPU
and AI models love GPUs.

00:07:05.330 --> 00:07:06.650
So lemme just show you real quick.

00:07:06.800 --> 00:07:10.400
I did install alama on a Linux
virtual machine and I'll just demo the

00:07:10.400 --> 00:07:13.850
performance for you real quick. By the
way, if you're running a Mac with an M1,

00:07:13.850 --> 00:07:17.060
M two or M three processor, it actually
works great. I forgot to install it.

00:07:17.090 --> 00:07:19.280
I got to install it real quick and
I'll ask you that same question.

00:07:19.280 --> 00:07:22.700
What is a pug? It's going to
take a minute, it'll still work,

00:07:22.760 --> 00:07:25.940
but it's going to be slower on CPUs and
there it goes. It didn't take too long,

00:07:25.940 --> 00:07:27.080
but notice it is a bit slower.

00:07:27.140 --> 00:07:30.560
Now if you're running WSL and you know
have an Nvidia GPU and it didn't show up,

00:07:31.040 --> 00:07:34.070
I'll show you in a minute how you can
get those drivers installed. But anyways,

00:07:34.100 --> 00:07:35.300
just sit back for a minute,

00:07:35.990 --> 00:07:38.510
sip your coffee and think
about how powerful this is.

00:07:38.540 --> 00:07:43.520
The tinfoil hat version of me
stinking loves this because let's say

00:07:43.520 --> 00:07:47.840
the zombie apocalypse happens, right?
The grid goes down, things are crazy,

00:07:47.930 --> 00:07:51.020
but as long as I have my
laptop and a solar panel,

00:07:51.170 --> 00:07:55.430
I still have AI and it can help
me survive the zombie apocalypse.

00:07:55.490 --> 00:07:58.550
Let's actually see how that would
work. It gives me next steps.

00:07:58.580 --> 00:08:01.730
I could have it help me with the water
filtration system. This is just cool,

00:08:01.760 --> 00:08:03.860
right? It's amazing. But can
I show you something funny?

00:08:04.190 --> 00:08:07.790
You may have caught this
earlier. Who is network? Chuck?

00:08:09.530 --> 00:08:14.270
What? Dude, I've always
wanted to be Rick Grimes.

00:08:14.390 --> 00:08:17.990
That is so fun, but seriously,
it kind of hallucinated there.

00:08:17.990 --> 00:08:19.670
It didn't have the correct information.

00:08:19.820 --> 00:08:23.600
It's so funny how it mixed the
zombie apocalypse prompt with me.

00:08:23.840 --> 00:08:26.660
I love that so much. Let's try
a different model. I'll say bye.

00:08:27.230 --> 00:08:30.050
I'll try a really fun one
called mytral. And by the way,

00:08:30.050 --> 00:08:33.560
if you want to know which ones you
can run with Llama, which LLMs,

00:08:33.800 --> 00:08:36.710
they get a page for their models right
here and all the ones you can run,

00:08:36.800 --> 00:08:39.230
including llama two,
uncensored Wizard Math.

00:08:39.290 --> 00:08:41.600
I might give that to my kids
actually. Let's see what it says.

00:08:41.600 --> 00:08:43.520
Now who is Network Chuck?

00:08:45.950 --> 00:08:50.000
Now my name is not Chuck Davis and my
YouTube channel is not called Network

00:08:50.000 --> 00:08:50.870
Chuck on Tech.

00:08:50.900 --> 00:08:54.410
So clearly the data this thing was trained
on is either not up to date or just

00:08:54.440 --> 00:08:56.760
plain wrong. So now the question is cool,

00:08:57.420 --> 00:09:01.650
we've got this local private ai,
this LLM, that's super powerful,

00:09:02.040 --> 00:09:04.950
but how do we teach it the
correct information for us?

00:09:05.070 --> 00:09:08.550
How can I teach it to know
that I'm network Chuck,
Chuck Keith, not Chuck Davis,

00:09:08.550 --> 00:09:09.780
and my channel is called Network Chuck.

00:09:09.840 --> 00:09:13.170
Or maybe I'm a business and I want it
to know more than just what's publicly

00:09:13.170 --> 00:09:16.485
available because sure, right
now if you downloaded this lm,

00:09:16.505 --> 00:09:17.850
you could probably use it in your job,

00:09:17.880 --> 00:09:22.020
but you can only go so far without it
knowing more about your job. For example,

00:09:22.020 --> 00:09:23.070
maybe you're on a help desk.

00:09:23.160 --> 00:09:27.210
Imagine if you could take your help
desk's knowledge base, your IT procedures,

00:09:27.360 --> 00:09:29.040
your documentation. Not only that,

00:09:29.045 --> 00:09:31.920
but maybe you have a database
of closed tickets, open tickets.

00:09:31.980 --> 00:09:35.520
If you could take all that data and
feed it to this LLM and then ask it

00:09:35.520 --> 00:09:38.220
questions about all of
that, that would be crazy.

00:09:38.400 --> 00:09:41.580
Or maybe you wanted to help troubleshoot
code that your company's written.

00:09:41.610 --> 00:09:44.760
You could even make this LM
public facing for your customers.

00:09:44.820 --> 00:09:47.970
You feed information about your product
and the customer could interact with

00:09:47.970 --> 00:09:49.260
that chat bot you make.

00:09:49.260 --> 00:09:53.730
Maybe this is all possible with a process
called fine tuning where we can train

00:09:53.730 --> 00:09:58.410
this AI on our own proprietary
secret private stuff about our

00:09:58.410 --> 00:10:00.570
company or maybe our lives or
whatever you want to use it for,

00:10:00.575 --> 00:10:01.560
whatever use case is,

00:10:01.740 --> 00:10:05.970
and this is fantastic because maybe before
you couldn't use a public LLM because

00:10:05.975 --> 00:10:08.160
you weren't allowed to share your
company's data with that LLM,

00:10:08.190 --> 00:10:10.680
whether it's compliance reasons or you
just simply didn't want to share that

00:10:10.680 --> 00:10:12.600
data because it's secret.
Whatever the case,

00:10:12.600 --> 00:10:15.720
it's possible now because
this AI is private,

00:10:15.810 --> 00:10:17.970
it's local and whatever
data you feed to it,

00:10:18.000 --> 00:10:20.730
it's going to stay right there in a
company. It's not leaving the door.

00:10:20.790 --> 00:10:24.900
That idea just makes me so excited
because I think it is the future of AI and

00:10:24.900 --> 00:10:28.320
how companies and individuals
will approach it. It's
going to be more private.

00:10:28.350 --> 00:10:31.890
Back to our question though,
fine tuning, that sounds cool.

00:10:31.980 --> 00:10:34.770
Training and AI on your own
data, but how does that work?

00:10:34.860 --> 00:10:38.370
Because as we saw before with
pre-training a model with mata,

00:10:38.580 --> 00:10:42.510
it took them 6,000 GPUs
over 1.7 million GPU hours.

00:10:42.750 --> 00:10:46.170
Do we have to have this massive
data center to make this happen? No.

00:10:46.350 --> 00:10:50.040
Check this out, and this is such a fun
example, VMware, they asked chat GPT,

00:10:50.190 --> 00:10:52.261
what's the latest version
of VMware vSphere?

00:10:52.350 --> 00:10:55.503
Now the latest chat GPT
knew about was vSphere 7.0,

00:10:55.508 --> 00:10:58.440
but that wasn't helpful to VMware because
their latest version they were working

00:10:58.440 --> 00:10:59.610
on chat hadn't been released yet.

00:10:59.615 --> 00:11:02.550
So it wasn't public knowledge
was vSphere eight update too.

00:11:02.640 --> 00:11:06.450
And they wanted information like this
internal information not yet released to

00:11:06.450 --> 00:11:07.020
the public.

00:11:07.020 --> 00:11:10.290
They wanted this to be available to
their internal team so they could ask

00:11:10.500 --> 00:11:14.160
something like chat GBT, Hey, what's
the latest version of vSphere?

00:11:14.280 --> 00:11:15.420
And they could answer correctly.

00:11:15.570 --> 00:11:19.470
So to do what VMware is trying to do
to fine tune a model or train it on new

00:11:19.470 --> 00:11:22.260
data, it does require a lot. First of all,

00:11:22.260 --> 00:11:24.390
you would need some
hardware servers with GPUs.

00:11:24.570 --> 00:11:29.220
Then you would also need a bunch of
tools and libraries and SDKs like PyTorch

00:11:29.220 --> 00:11:33.360
and TensorFlow, pandas, MPI side
kit, learn transformers and fast ai.

00:11:33.420 --> 00:11:34.200
The list goes on.

00:11:34.200 --> 00:11:37.860
You need lots of tools and resources
in order to fine tune an LLM.

00:11:37.920 --> 00:11:40.590
That's why I'm a massive fan of
what VMware is doing right here.

00:11:40.920 --> 00:11:44.100
They have something called the
VMware private AI with Nvidia,

00:11:44.430 --> 00:11:49.080
the gajillion things I just listed
off. They include in one package,

00:11:49.200 --> 00:11:53.160
one combo meal, a recipe of
ai, fine tuning goodness.

00:11:53.320 --> 00:11:57.310
So as a company it becomes a bit easier
to do this stuff yourself locally.

00:11:57.340 --> 00:12:00.280
For the system engineer you have on
staff who knows VMware and loves it,

00:12:00.370 --> 00:12:01.330
they could do this stuff,

00:12:01.390 --> 00:12:04.180
they could implement this and the data
scientists they have on staff that will

00:12:04.185 --> 00:12:07.390
actually do some of the fine tuning,
all the tools are right there.

00:12:07.450 --> 00:12:10.330
So here's what it looks like to fine tune
and we're going to kind of peek behind

00:12:10.330 --> 00:12:12.550
the curtain at what a data
scientist actually does.

00:12:12.700 --> 00:12:16.720
So first we have the infrastructure
and we start here in vSphere, VMware.

00:12:17.080 --> 00:12:20.470
Now if you don't know what vSphere
is or VMware, think virtual machines,

00:12:20.530 --> 00:12:23.590
you got one big physical server. The
hardware, the stuff you can feel,

00:12:23.590 --> 00:12:26.200
touch and smell. You haven't smelled
the server, I dunno what you're doing.

00:12:26.260 --> 00:12:29.620
And instead of installing one operating
system on them like Windows or Linux,

00:12:29.830 --> 00:12:31.750
you install VMware's, EA XI,

00:12:31.900 --> 00:12:35.530
which will then allow you to virtualize
or create a bunch of additional virtual

00:12:35.530 --> 00:12:37.270
computers. So instead of one computer,

00:12:37.270 --> 00:12:40.420
you've got a bunch of computers all
using the same hardware resources.

00:12:40.450 --> 00:12:43.270
And that's what we have right here.
One of those virtual computers,

00:12:43.570 --> 00:12:44.410
a virtual machine.

00:12:44.500 --> 00:12:49.330
This by the way is one of their special
deep learning VMs that has all the tools

00:12:49.330 --> 00:12:53.620
I mentioned and many, many more
pre-installed, ready to go.

00:12:53.710 --> 00:12:55.270
Everything a data scientist could love.

00:12:55.300 --> 00:12:59.200
It's kind of like a surgeon walking in
to do some surgery and like their doctor

00:12:59.200 --> 00:13:01.870
assistants or whatever have
prepared all their tools.

00:13:01.870 --> 00:13:04.990
It's all in the tray laid out
nice and neat to the surgeon.

00:13:04.990 --> 00:13:07.900
All he has to do is walk
in and just go scalpel.

00:13:08.410 --> 00:13:10.210
That's what we're doing
here for the data scientist.

00:13:10.270 --> 00:13:11.440
Now talking more about hardware,

00:13:11.440 --> 00:13:16.330
this guy has a couple Nvidia GPUs assigned
to it or pass through to it through

00:13:16.540 --> 00:13:20.020
a technology called PCIE Passthrough.
These are some beefy GPUs.

00:13:20.080 --> 00:13:25.030
I notice they are V GPU for virtual GPU
similar to what you do with the CPU,

00:13:25.180 --> 00:13:29.980
cutting up the PU and assigning some
of that to a virtual CPU on a virtual

00:13:29.980 --> 00:13:33.280
machine. So here we are in data scientists
world. This is a Jupiter notebook,

00:13:33.310 --> 00:13:35.230
a common tool used by a data scientist,

00:13:35.260 --> 00:13:37.930
and what you're going to see here is a
lot of code that they're using to prepare

00:13:37.930 --> 00:13:38.500
the data,

00:13:38.500 --> 00:13:42.190
specifically the data that they're
going to train or fine tune the existing

00:13:42.190 --> 00:13:44.020
model on. Now we're not
going to dive deep on that,

00:13:44.020 --> 00:13:45.580
but I do want you to see
this, check this out.

00:13:45.670 --> 00:13:48.490
A lot of this code is all about getting
the data ready. So in VMware's case,

00:13:48.490 --> 00:13:51.550
it might be a bunch of the knowledge
base product documentation and they're

00:13:51.550 --> 00:13:55.360
getting it ready to be fed to the LLM.
And here's what I wanted you to see.

00:13:55.600 --> 00:13:59.650
Here's the dataset that we're training
this model on. We're fine tuning.

00:13:59.680 --> 00:14:04.390
We only have 9,800 examples that we're
giving it or 9,800 new prompts or

00:14:04.395 --> 00:14:06.760
pieces of data. And that
data might look like this,

00:14:06.880 --> 00:14:11.860
like a simple question or a prompt and
then we feed it the correct answer and

00:14:11.860 --> 00:14:14.050
that's how we essentially
train ai. But again,

00:14:14.050 --> 00:14:16.060
we're only giving it 9,800 examples,

00:14:16.065 --> 00:14:20.950
which is not a lot at all and is
extremely small compared to how the

00:14:20.950 --> 00:14:22.210
model was originally trained.

00:14:22.270 --> 00:14:25.690
And I point that out to say that we're
not going to need a ton of hardware or a

00:14:25.690 --> 00:14:28.180
ton of resources to fine tune this model.

00:14:28.210 --> 00:14:32.260
We won't need the 6,000 GPUs we needed
for MATA to originally create this model.

00:14:32.260 --> 00:14:33.340
We're just adding to it,

00:14:33.345 --> 00:14:37.630
changing some things or fine tuning it
to what our use case is and looking at

00:14:37.630 --> 00:14:41.020
what actually will be changed
when we run this and we train it,

00:14:41.170 --> 00:14:46.120
we're only changing 65 million parameters,
which sounds like a lot, right?

00:14:46.480 --> 00:14:49.810
But not in the grand scheme of things
of like a 7 billion parameter model.

00:14:49.930 --> 00:14:52.640
We're only changing 0.93% of the model.

00:14:52.850 --> 00:14:54.380
And then we can actually
run our fine tuning,

00:14:54.440 --> 00:14:58.010
which this is a specific technique in
fine tuning called prompt tuning where we

00:14:58.010 --> 00:15:02.360
simply feed up additional prompts with
answers to change how it'll react to

00:15:02.360 --> 00:15:03.680
people asking you questions.

00:15:03.770 --> 00:15:06.650
This process will take three to four
minutes to fine tune it because again,

00:15:06.650 --> 00:15:10.820
we're not changing a lot and that is
just so super powerful and I think VMware

00:15:10.820 --> 00:15:12.680
is leading the charge with private ai.

00:15:12.890 --> 00:15:16.965
VMware and Nvidia take all the guesswork
out of getting things set up to fine

00:15:17.030 --> 00:15:19.550
tune an LLM. They've
got deep learning VMs,

00:15:19.610 --> 00:15:23.570
which are insane VMs that
come pre-installed with
everything you could want

00:15:23.630 --> 00:15:26.660
everything a data scientist
would need to find tune an LLM.

00:15:26.750 --> 00:15:29.870
Then Nvidia has an entire suite
of tools sensor around their GPUs,

00:15:29.990 --> 00:15:33.560
taking advantage of some really exciting
things to help you fine tune your lms.

00:15:33.800 --> 00:15:36.680
Now there's one thing I didn't talk about
because I wanted to save it for last.

00:15:36.710 --> 00:15:39.950
For right now it's this right
here, this vector database,

00:15:39.950 --> 00:15:42.590
post gray SQL box here.

00:15:42.800 --> 00:15:46.850
This is something called rag and it's
what we're about to do with our own

00:15:46.850 --> 00:15:51.140
personal GPT here in a bit. Retrieval,
augment the generation. So scenario,

00:15:51.200 --> 00:15:54.260
let's say you have a database of
product information, internal docs,

00:15:54.260 --> 00:15:58.610
whatever it is, and you haven't fine
tuned your LLM on this just yet.

00:15:58.760 --> 00:16:01.640
So it doesn't know about it. You
don't have to do that with rag.

00:16:01.670 --> 00:16:05.660
You can connect your LLM to
this database of information,

00:16:05.870 --> 00:16:08.240
this knowledge base and
give it these instructions.

00:16:08.270 --> 00:16:11.360
Say whenever I ask you a question about
any of the things in this database,

00:16:11.450 --> 00:16:13.580
before you answer, consult the database,

00:16:13.760 --> 00:16:16.310
go look at it and make sure
what you're saying is accurate.

00:16:16.400 --> 00:16:20.060
We're not retraining the LLM, we're
just saying, Hey, before you answer,

00:16:20.090 --> 00:16:23.330
go check real quick in this database to
make sure it's accurate to make sure you

00:16:23.330 --> 00:16:25.760
got your stuff right.
Isn't that cool? So yes,

00:16:25.760 --> 00:16:29.240
fine tuning is cool and training
an LLM on your own data is awesome,

00:16:29.270 --> 00:16:31.340
but in between those
moments of fine tuning,

00:16:31.370 --> 00:16:34.340
you can have rag set up where
it can consult your database,

00:16:34.400 --> 00:16:38.330
your internal documentation and give
correct answers based on what you have in

00:16:38.330 --> 00:16:40.940
that database. That is so stinking cool.

00:16:40.970 --> 00:16:43.550
So with VMware private AI
foundation with nvidia,

00:16:43.640 --> 00:16:47.600
they have those tools baked right in
to where it just kind of works for what

00:16:47.605 --> 00:16:51.500
would otherwise be a very complex setup.
And by the way, this whole rag thing,

00:16:51.740 --> 00:16:53.750
like I said earlier,
we're about to do this,

00:16:53.840 --> 00:16:58.610
I actually connected a lot of my notes
and journal entries to a private GPT

00:16:58.610 --> 00:17:03.590
using RAG and I was able to talk
with it about me asking it about my

00:17:03.590 --> 00:17:07.730
journal entries and answering questions
about my past. That's so powerful. Now,

00:17:07.730 --> 00:17:08.330
before we move on,

00:17:08.330 --> 00:17:12.020
I just want to highlight the fact that
Nvidia with their Nvidia AI enterprise

00:17:12.710 --> 00:17:17.090
gives you some amazing and fantastic
tools to pull the LLM of your choice and

00:17:17.090 --> 00:17:21.140
then fine tune and customize and deploy
that LLM. It's all built in right here.

00:17:21.200 --> 00:17:22.460
So VMware Cloud Foundation,

00:17:22.460 --> 00:17:26.360
they provide the robust infrastructure
and NVIDIA provides all the amazing AI

00:17:26.360 --> 00:17:29.360
tools you need to develop
and deploy these custom LLMs.

00:17:29.390 --> 00:17:31.730
Now it's not just Nvidia, they're
partnering with Intel as well.

00:17:31.730 --> 00:17:34.130
So VMware is covering all the
tools that admins care about.

00:17:34.190 --> 00:17:36.050
And then for the data
scientists, this is for you.

00:17:36.050 --> 00:17:38.330
Intel's got your back data analytics,

00:17:38.360 --> 00:17:42.530
generative AI and deep learning tools
and some classic ML or machine learning.

00:17:42.620 --> 00:17:46.370
And they're also working with IBM, all
you IBM fans. You can do this too. Again,

00:17:46.370 --> 00:17:49.770
VMware has the admin's back. But
for the data scientist, Watson,

00:17:49.770 --> 00:17:52.740
one of the first AI things I ever
heard about Red Hat and OpenShift,

00:17:52.800 --> 00:17:55.350
and I love this because what VMware
is doing is all about choice.

00:17:55.440 --> 00:17:58.170
If you want to run your own
local private ai, you can.

00:17:58.170 --> 00:18:00.630
You're not just stuck with one of the
big guys out there and you can choose to

00:18:00.630 --> 00:18:04.770
run it with Nvidia and VMware,
Intel and VMware, IBM and VMware.

00:18:04.800 --> 00:18:06.540
You got options. So there's
nothing stopping you.

00:18:06.545 --> 00:18:09.810
It's not for some of the bonus section
of this video and that's how to run your

00:18:09.840 --> 00:18:14.070
own private GPT with your own
knowledge base. Now, fair warning,

00:18:14.070 --> 00:18:16.110
it is a bit more advanced,
but if you stick with me,

00:18:16.110 --> 00:18:19.440
you should be able to get this up and
running. So take one more sip of coffee.

00:18:20.190 --> 00:18:22.770
Let's get this going. Now, first of
all, this will not be using a lama.

00:18:23.070 --> 00:18:26.400
This will be a separate project
called Private GPT. Now disclaimer,

00:18:26.400 --> 00:18:29.340
this is kind of hard to do.
Unlike VMware private ai,

00:18:29.340 --> 00:18:30.720
which they do it all for you,

00:18:30.750 --> 00:18:34.110
it's a complete solution for companies
to run their own private local ai.

00:18:34.200 --> 00:18:37.680
What I'm about to show you is not that
at all. No affiliation with VMware.

00:18:37.920 --> 00:18:39.510
It's a free side project.

00:18:39.570 --> 00:18:44.250
You can try just to get a little taste
of what running your own private GPT with

00:18:44.250 --> 00:18:47.700
rag tastes like. Did I do
that right? I don't know.

00:18:47.910 --> 00:18:51.000
Now L Martinez has a great doc on
how to install this. It's a lot,

00:18:51.000 --> 00:18:53.670
but you can do it. And if
you just want a quick start,

00:18:53.670 --> 00:18:57.150
he does have a few lines of code for
Linux and Mac users. Fair warning,

00:18:57.150 --> 00:19:00.570
this is CPU only. You can't really
take advantage of RAG without A GPU,

00:19:00.750 --> 00:19:03.450
which is what I wanted to do. So
here's my very specific scenario.

00:19:03.480 --> 00:19:06.870
I've got a Windows PC with an
NVIDIA 40 90. How do I run this?

00:19:06.870 --> 00:19:11.640
Linux-based project. WSL, and I'm so
thankful to this guy Emelia Lance a lot.

00:19:11.730 --> 00:19:13.890
He put an entire guide
together of how to set this up.

00:19:14.310 --> 00:19:17.730
I'm not going to walk you through every
step because he already did that link

00:19:17.730 --> 00:19:20.310
below, but I seriously need to buy
this guy a coffee. How do I do that?

00:19:20.400 --> 00:19:22.980
I don't know, Emil, if you're
watching this, reach out to me.

00:19:22.980 --> 00:19:24.270
I'll send you some coffee. So anyways,

00:19:24.270 --> 00:19:27.630
I went through every step from installing
all the prereqs to installing NVIDIA

00:19:27.630 --> 00:19:31.170
drivers and using poetry to handle
dependencies, which poetry is pretty cool.

00:19:31.290 --> 00:19:32.123
I landed here.

00:19:32.220 --> 00:19:36.600
I've got a private local working private
GPT that I can access through my web

00:19:36.600 --> 00:19:38.910
browser and it's using my GPU,
which is pretty cool. Now,

00:19:38.910 --> 00:19:40.590
first I try a simple document upload,

00:19:40.800 --> 00:19:43.290
got this VMware article that details
a lot of what we talked about in this

00:19:43.290 --> 00:19:46.380
video. I upload it and I start asking
you questions about this article.

00:19:46.440 --> 00:19:49.860
I tried something specific like show me
something about VMware AI market growth.

00:19:50.280 --> 00:19:52.500
Bam, it figured it out,
it told me. Then I'm like,

00:19:52.500 --> 00:19:54.330
what's the coolest thing
about VMware private ai?

00:19:55.290 --> 00:19:58.140
It told me I'm sitting here chatting
with a document, but then I'm like,

00:19:58.145 --> 00:20:00.330
let's try something bigger. I
want to chat with my journals.

00:20:00.480 --> 00:20:03.720
I've got a ton of journals on markdown
format and I want to ask you questions

00:20:03.725 --> 00:20:06.360
about me. Now this specific step
is not covered in the article.

00:20:06.360 --> 00:20:07.590
So here's how you do it. First,

00:20:07.595 --> 00:20:10.740
you'll want to grab your
folder of whatever documents
you want to ask questions

00:20:10.740 --> 00:20:12.600
about and throw it onto your machine.

00:20:12.720 --> 00:20:16.680
So I copied over to my WSL machine and
then I ingested it with this command once

00:20:16.680 --> 00:20:18.780
complete and I ran private GPT. Again,

00:20:18.840 --> 00:20:21.090
here's all my documents and
I'm ready to ask it questions.

00:20:21.120 --> 00:20:25.980
So let's test this out. I'm going
to ask it what did I do in takayama?

00:20:26.490 --> 00:20:31.170
So I went to Japan in November of 2023.
Let's see if you can search my notes,

00:20:31.230 --> 00:20:33.060
figure out when that was and what I did.

00:20:36.300 --> 00:20:40.800
That's awesome. Oh my goodness.

00:20:41.190 --> 00:20:44.880
Let's see, what did I eat in Tokyo?

00:20:45.600 --> 00:20:49.480
How cool is that? Oh my gosh,
that's so fun. No, it's not perfect,

00:20:49.510 --> 00:20:53.890
but I can see the potential here.
That's insane. I love this so much.

00:20:53.920 --> 00:20:57.100
Private AI is the future and that's why
we're seeing VMware bring products like

00:20:57.100 --> 00:21:01.030
this to companies to run their own
private local AI and then make it pretty

00:21:01.030 --> 00:21:04.180
easy. If you actually did that private
GPT thing, that little side project,

00:21:04.750 --> 00:21:07.780
there's a lot to it. Lots of tools you
have to install, it's kind of a pain.

00:21:07.810 --> 00:21:08.500
But with VMware,

00:21:08.500 --> 00:21:11.530
they kind of cover everything like that
deep learning VM they offer as part of

00:21:11.530 --> 00:21:15.220
their solution. It's got all the
tools ready to go. Pre-baked again,

00:21:15.225 --> 00:21:17.650
you're like a surgeon just
walking in saying scalpel.

00:21:17.770 --> 00:21:20.530
You got all this stuff right there. So
if you want to bring AI to your company,

00:21:20.535 --> 00:21:24.580
check out VMware private AI link below
and thank you to VMware by Broadcom for

00:21:24.580 --> 00:21:28.420
sponsoring this video. You made it to
the end of the video time for a quiz.

00:21:28.510 --> 00:21:32.020
This quiz will test the knowledge you've
gained in this video and the first five

00:21:32.025 --> 00:21:36.040
people to get a hundred percent on this
quiz will get free coffee from Network

00:21:36.040 --> 00:21:38.590
Chuck Coffee. So here's how
you take the quiz right now.

00:21:38.590 --> 00:21:40.810
Check the description in your
video and click on this link.

00:21:41.170 --> 00:21:43.720
If you're not currently signed into the
academy, go ahead and get signed in.

00:21:43.930 --> 00:21:47.140
If you're not a member, go ahead
and click on sign off. It's free.

00:21:47.350 --> 00:21:48.105
Once you're signed in,

00:21:48.105 --> 00:21:51.010
it will take you to your dashboard showing
you all the stuff you have access to

00:21:51.010 --> 00:21:54.040
with your free academy account.
But to get right back to that quiz,

00:21:54.160 --> 00:21:55.330
go back to the YouTube video,

00:21:55.390 --> 00:21:58.030
click on that link once more and
it should take you right to it.

00:21:58.390 --> 00:22:02.530
Go ahead and click on start now and
start your quiz. Here's a little preview.

00:22:03.070 --> 00:22:05.980
That's it. The first five to get
a hundred percent free coffee.

00:22:06.100 --> 00:22:06.910
If you're one of the five,

00:22:06.910 --> 00:22:09.760
you'll know because you'll
receive an email with free coffee.

00:22:09.850 --> 00:22:12.670
You got to be quick, you got to be smart.
I'll see you guys in the next video.

